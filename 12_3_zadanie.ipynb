{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6aFtMO_UwyfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d13ba4c-2c55-4a5b-8e69-2f950d2ca9bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dane: Digits\n",
            "\n",
            "Digits - Logistic Regression (train):\n",
            "Accuracy : 1.0000\n",
            "Precision: 1.0000\n",
            "Recall   : 1.0000\n",
            "F1-score : 1.0000\n",
            "\n",
            "Digits - Logistic Regression (test):\n",
            "Accuracy : 0.9685\n",
            "Precision: 0.9692\n",
            "Recall   : 0.9685\n",
            "F1-score : 0.9686\n",
            "\n",
            "Digits - Random Forest (train):\n",
            "Accuracy : 1.0000\n",
            "Precision: 1.0000\n",
            "Recall   : 1.0000\n",
            "F1-score : 1.0000\n",
            "\n",
            "Digits - Random Forest (test):\n",
            "Accuracy : 0.9759\n",
            "Precision: 0.9760\n",
            "Recall   : 0.9759\n",
            "F1-score : 0.9759\n",
            "\n",
            "Digits - SVM (train):\n",
            "Accuracy : 0.9968\n",
            "Precision: 0.9968\n",
            "Recall   : 0.9968\n",
            "F1-score : 0.9968\n",
            "\n",
            "Digits - SVM (test):\n",
            "Accuracy : 0.9870\n",
            "Precision: 0.9871\n",
            "Recall   : 0.9870\n",
            "F1-score : 0.9870\n",
            "\n",
            " Dane: Wine\n",
            "\n",
            "Wine - Logistic Regression (train):\n",
            "Accuracy : 0.9919\n",
            "Precision: 0.9921\n",
            "Recall   : 0.9919\n",
            "F1-score : 0.9919\n",
            "\n",
            "Wine - Logistic Regression (test):\n",
            "Accuracy : 1.0000\n",
            "Precision: 1.0000\n",
            "Recall   : 1.0000\n",
            "F1-score : 1.0000\n",
            "\n",
            "Wine - Random Forest (train):\n",
            "Accuracy : 1.0000\n",
            "Precision: 1.0000\n",
            "Recall   : 1.0000\n",
            "F1-score : 1.0000\n",
            "\n",
            "Wine - Random Forest (test):\n",
            "Accuracy : 1.0000\n",
            "Precision: 1.0000\n",
            "Recall   : 1.0000\n",
            "F1-score : 1.0000\n",
            "\n",
            "Wine - SVM (train):\n",
            "Accuracy : 0.6694\n",
            "Precision: 0.6411\n",
            "Recall   : 0.6694\n",
            "F1-score : 0.6368\n",
            "\n",
            "Wine - SVM (test):\n",
            "Accuracy : 0.7593\n",
            "Precision: 0.7537\n",
            "Recall   : 0.7593\n",
            "F1-score : 0.7235\n",
            "\n",
            "RAPORT KOŃCOWY\n",
            "Dane: 70% trening / 30% test, random_state=42\n",
            "Modele: Regresja logistyczna, Las losowy, SVM\n",
            "\n",
            "Podsumowanie: Dane Digits\n",
            "- Logistic Regression: train: 1.0000, test: 0.9685\n",
            "  Wniosek: Mała różnica – wyniki akceptowalne.\n",
            "- Random Forest: train: 1.0000, test: 0.9759\n",
            "  Wniosek: Mała różnica – wyniki akceptowalne.\n",
            "- SVM: train: 0.9968, test: 0.9870\n",
            "  Wniosek: Model dobrze radzi sobie z nowymi danymi.\n",
            "\n",
            "Podsumowanie: Dane Wine\n",
            "- Logistic Regression: train: 0.9919, test: 1.0000\n",
            "  Wniosek: Model dobrze radzi sobie z nowymi danymi.\n",
            "- Random Forest: train: 1.0000, test: 1.0000\n",
            "  Wniosek: Model dobrze radzi sobie z nowymi danymi.\n",
            "- SVM: train: 0.6694, test: 0.7593\n",
            "  Wniosek: Mała różnica – wyniki akceptowalne.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_digits, load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Obliczanie metryk\n",
        "def ocen_model(y_true, y_pred, zbior, model, etap):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\n{zbior} - {model} ({etap}):\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall   : {rec:.4f}\")\n",
        "    print(f\"F1-score : {f1:.4f}\")\n",
        "\n",
        "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}\n",
        "\n",
        "# Trening\n",
        "def trenuj_model(model, X_tr, X_te, y_tr, y_te, zbior, nazwa):\n",
        "    model.fit(X_tr, y_tr)\n",
        "    wyniki_train = ocen_model(y_tr, model.predict(X_tr), zbior, nazwa, 'train')\n",
        "    wyniki_test = ocen_model(y_te, model.predict(X_te), zbior, nazwa, 'test')\n",
        "    return {'train': wyniki_train, 'test': wyniki_test}\n",
        "\n",
        "# Wczytanie i podział danych\n",
        "digits = load_digits()\n",
        "wine = load_wine()\n",
        "\n",
        "X_dig_tr, X_dig_te, y_dig_tr, y_dig_te = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
        "X_win_tr, X_win_te, y_win_tr, y_win_te = train_test_split(wine.data, wine.target, test_size=0.3, random_state=42)\n",
        "\n",
        "modele = [\n",
        "    ('Logistic Regression', LogisticRegression(max_iter=5000, random_state=42)),\n",
        "    ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('SVM', SVC(kernel='rbf', random_state=42))\n",
        "]\n",
        "\n",
        "# Trenowanie i zbieranie wyników\n",
        "print(\"Dane: Digits\")\n",
        "wyniki_digits = {nazwa: trenuj_model(model, X_dig_tr, X_dig_te, y_dig_tr, y_dig_te, 'Digits', nazwa) for nazwa, model in modele}\n",
        "\n",
        "print(\"\\n Dane: Wine\")\n",
        "wyniki_wine = {nazwa: trenuj_model(model, X_win_tr, X_win_te, y_win_tr, y_win_te, 'Wine', nazwa) for nazwa, model in modele}\n",
        "\n",
        "# Podsumowanie wyników\n",
        "def podsumuj(wyniki, zbior):\n",
        "    print(f\"\\nPodsumowanie: {zbior}\")\n",
        "    for nazwa, dane in wyniki.items():\n",
        "        acc_tr = dane['train']['accuracy']\n",
        "        acc_te = dane['test']['accuracy']\n",
        "        print(f\"- {nazwa}: train: {acc_tr:.4f}, test: {acc_te:.4f}\")\n",
        "        roznica = acc_tr - acc_te\n",
        "        if roznica > 0.05:\n",
        "            print(\"  Wniosek: Możliwe przeuczenie (overfitting).\")\n",
        "        elif abs(roznica) < 0.02:\n",
        "            print(\"  Wniosek: Model dobrze radzi sobie z nowymi danymi.\")\n",
        "        else:\n",
        "            print(\"  Wniosek: Mała różnica – wyniki akceptowalne.\")\n",
        "\n",
        "# Wypisanie raportu\n",
        "print(\"\\nRAPORT KOŃCOWY\")\n",
        "print(\"Dane: 70% trening / 30% test, random_state=42\")\n",
        "print(\"Modele: Regresja logistyczna, Las losowy, SVM\")\n",
        "\n",
        "podsumuj(wyniki_digits, \"Dane Digits\")\n",
        "podsumuj(wyniki_wine, \"Dane Wine\")\n"
      ]
    }
  ]
}